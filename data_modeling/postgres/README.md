## Data Modeling with Postgres - Sparkify 

### Introduction 

<i>Sparkify</i> is a startup that owns a new music streaming app.
They've collected some data about songs and user activity on this app. All their data resides in two directories of JSON files: 

- Logs  on user activities;
- Songs in their app.

<i>Sparkify</i> needs to create a Postgres database to optimize queries on song play analysis. 

### Objectives

Build an ETL pipeline using Python from JSON files to Postgres and define fact and dimension tables for a star schema.

### Datasets

- **Songs**: is a subset of real data from [Million Song Dataset](http://millionsongdataset.com/). The file looks like this:

```
 {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0} </code>
 ```

- **Logs**: generated by [event simulator](https://github.com/Interana/eventsim). The file looks like this:

```
 {"artist":"Girl Talk","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":8,"lastName":"Summers","length":160.15628,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"Once again","status":200,"ts":1541107734796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
 ```

 ### Data Modeling

 A Star Schema was created optmized for queries with both datasets.

 - **Fact Table**: 

   - **songplays**: records in log data with song plays(records with page ```NextSong```). Columns: *song_play_id*, *start_time*, *user_id*, *level*, *song_id*, *artist_id*, *session_id*, *location*, *user_agent*.

- **Dimension Tables**:

   - **users**: users in the app. Columns: *user_id*, *first_name*, *last_name*, *gender*, *level*.

   - **songs**: songs in the app. Columns: *song_id*, *artist_id*, *year*, *duration*.

   - **artists**: artists in the app. Columns: *artist_id*, *name*, *location*, *latitude*, *longitude*.

   - **time**: timestamps of records in songplays. Columns: *start_time*, *hour*, *day*, *week*, *month*, *year*, *weekday*.

### Execution Steps
1. ```\data\``` contains all datasets used in this project.

2. Run ```create_tables.py``` to drop and create database sparkifydb and tables in Star Schema.  
 You can also run this file to reset tables before ETL scripts:

```
python create_tables.py
```
**Assuming that you have a postgres server running in localhost as the file shows. You can change *host, dbname, user and password* to work fine in your local machine.**

3. The ```etl.ipynb ``` file reads a single file from each ```song_data``` and ```log_data```. It loads data into our tales as a test. You can use, for example, Jupyter Notebook or Visual Code to open this file.

4. The ```sql_queries.py ``` file has all the queries used to **DROP**, **CREATE** and **INSERT**. This file is imported inside ```create_tables.py```, ```etl.ipynb ``` and ```etl.py ```.

5. The ```test.ipynb ``` file can display some basic queries of each table to check if your data has loaded right.

6. Finally, ```etl.py ``` contains all ETL processes to load from ```song_data``` and ```log_data``` and, most important, fill songplays fact table. Run in console:
```
python etl.py
```


















